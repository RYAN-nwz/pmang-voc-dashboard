# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import plotly.express as px
from datetime import datetime, timedelta
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re

# --- 1. ê¸°ë³¸ ì„¤ì • ---
# [ìˆ˜ì •] SPREADSHEET_IDë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì¼ íŒŒì¼ ë°©ì‹ìœ¼ë¡œ ë³µêµ¬í•©ë‹ˆë‹¤.
SPREADSHEET_ID = "1rgR21yUBtKSJKE4KYdBvoZzDU-0jZ_wLb6Tqc_zS7RM"
SERVICE_ACCOUNT_FILE = 'service_account.json'

st.set_page_config(page_title="í”¼ë§ ì›¹ë³´ë“œ VOC ëŒ€ì‹œë³´ë“œ", page_icon="ğŸ“Š", layout="wide")

# L2 íƒœê·¸ë¥¼ L1 ëŒ€ë¶„ë¥˜ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
L2_TO_L1_MAPPING = {
    'ë¡œê·¸ì¸/ì¸ì¦': 'ê³„ì •', 'ì •ë³´ ê´€ë¦¬': 'ê³„ì •',
    'ê¸°ìˆ  ì˜¤ë¥˜': 'ì‹œìŠ¤í…œ/í™˜ê²½',
    'ê²°ì œ ì˜¤ë¥˜/ë¯¸ì§€ê¸‰': 'ì¬í™”/ê²°ì œ', 'í™˜ë¶ˆ/ì²­ì•½ì² íšŒ': 'ì¬í™”/ê²°ì œ', 'ì¬í™” ì†Œì‹¤/ì˜¤ë¥˜': 'ì¬í™”/ê²°ì œ',
    'í´ë˜ìŠ¤/êµ¬ë… ìƒí’ˆ': 'ì¬í™”/ê²°ì œ', 'ì¬í™” ì •ì±…/í•œë„': 'ì¬í™”/ê²°ì œ',
    'ë°¸ëŸ°ìŠ¤/ë¶ˆë§Œ (íŒ¨ëª°ë¦¼)': 'ê²Œì„ í”Œë ˆì´', 'ì½˜í…ì¸  ì˜¤ë¥˜/ë¬¸ì˜': 'ê²Œì„ í”Œë ˆì´', 'í† ë„ˆë¨¼íŠ¸/ëŒ€íšŒ': 'ê²Œì„ í”Œë ˆì´',
    'ì ë ¹ì „/ê±°ì ì „': 'ê²Œì„ í”Œë ˆì´', 'ë­í‚¹í˜ìŠ¤íƒ€': 'ê²Œì„ í”Œë ˆì´', 'ì—°ìŠ¹ì±Œë¦°ì§€': 'ê²Œì„ í”Œë ˆì´', 'íŒ¨ë°€ë¦¬ê²Œì„': 'ê²Œì„ í”Œë ˆì´',
    'ê´‘ê³ /ë¬´ë£Œì¶©ì „ì†Œ': 'ì´ë²¤íŠ¸/í˜œíƒ', 'ì´ë²¤íŠ¸': 'ì´ë²¤íŠ¸/í˜œíƒ',
    'ë¹„ë§¤ë„ˆ/ìš•ì„¤ ì‹ ê³ ': 'ë¶ˆëŸ‰ ì´ìš©ì', 'ì œì¬ ë¬¸ì˜': 'ë¶ˆëŸ‰ ì´ìš©ì',
    'ì½˜í…ì¸ /ì‹œìŠ¤í…œ ê±´ì˜': 'ì •ì±…/ê±´ì˜ (VOC)', 'ìš´ì˜/ì •ì±… ê±´ì˜': 'ì •ì±…/ê±´ì˜ (VOC)',
    'ë‹¨ìˆœ ë¬¸ì˜/ë¯¸ë¶„ë¥˜': 'ê¸°íƒ€'
}

# --- 2. ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ ---

def classify_game(category):
    """'ì ‘ìˆ˜ ì¹´í…Œê³ ë¦¬'ì—ì„œ í‘œì¤€ ê²Œì„ ì´ë¦„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    if pd.isna(category): return "ê¸°íƒ€"
    category = str(category).lower()
    if "ë‰´ë§ê³ " in category or "newmatgo" in category: return "ë‰´ë§ê³ "
    if "ì„¯ë‹¤" in category or "sutda" in category: return "ì„¯ë‹¤"
    if "í¬ì»¤" in category or "poker" in category: return "í¬ì»¤"
    if "ì‡¼ë‹¤ìš´í™€ë¤" in category or "showdown" in category: return "ì‡¼ë‹¤ìš´í™€ë¤"
    if "ë‰´ë² ê°€ìŠ¤" in category or "newvegas" in category: return "ë‰´ë² ê°€ìŠ¤"
    return "ê¸°íƒ€"

def classify_platform(category):
    """'ì ‘ìˆ˜ ì¹´í…Œê³ ë¦¬'ì—ì„œ í”Œë«í¼ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    if pd.isna(category): return "ê¸°íƒ€"
    category = str(category).lower()
    if "for kakao" in category: return "for kakao"
    if "mob" in category or "ëª¨ë°”ì¼" in category: return "MOB"
    if "pc" in category: return "PC"
    return "ê¸°íƒ€"

def extract_gsn_usn(row):
    """'ë¬¸ì˜ë‚´ìš©' ë˜ëŠ” 'ê³ ê°ì •ë³´'ì—ì„œ GSN(ëª¨ë°”ì¼) ë˜ëŠ” USN(PC)ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    platform = row.get('í”Œë«í¼', '')
    
    if platform in ['MOB', 'for kakao']:
        inquiry_content = str(row.get('ë¬¸ì˜ë‚´ìš©', ''))
        gsn_match = re.search(r'íšŒì›ë²ˆí˜¸\s*:\s*(\d+)', inquiry_content)
        if gsn_match:
            return gsn_match.group(1)
            
    if platform == 'PC':
        customer_info = str(row.get('ê³ ê°ì •ë³´', ''))
        usn_match = re.search(r'\d+', customer_info)
        if usn_match:
            return usn_match.group(0)
            
    return ""

def extract_device_info(row):
    """'ë¬¸ì˜ë‚´ìš©'ì—ì„œ ê¸°ê¸°ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ í”Œë«í¼ì— ë”°ë¼ PCë¡œ ì§€ì •í•˜ëŠ” í•¨ìˆ˜"""
    inquiry_content = str(row.get('ë¬¸ì˜ë‚´ìš©', ''))
    device_match = re.search(r'íœ´ëŒ€í°ê¸°ê¸°ì •ë³´\s*:\s*(\S+)', inquiry_content)
    if device_match:
        return device_match.group(1)
    
    platform = row.get('í”Œë«í¼', '')
    if platform == 'PC':
        return 'PC'
        
    return ""

def truncate_inquiry_content(text):
    """ë¬¸ì˜ ë‚´ìš©ì—ì„œ ì •í˜•í™”ëœ í…œí”Œë¦¿ ë¶€ë¶„ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜"""
    if isinstance(text, str):
        return text.split("íšŒì›ë²ˆí˜¸ :")[0].strip()
    return ""

@st.cache_data(ttl=600)
def load_data():
    """[ìˆ˜ì •] Google Sheetsì—ì„œ 'ë‹¨ì¼ íŒŒì¼'ì˜ ëª¨ë“  ì‹œíŠ¸ë¥¼ ì½ì–´ì˜´"""
    try:
        scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
        
        # --- [ìˆ˜ì •] Secrets ëŒ€ì‹  ë¡œì»¬ íŒŒì¼ ì¸ì¦ìœ¼ë¡œ ë³µê·€ ---
        try:
            creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, scope)
            gc = gspread.authorize(creds)
            print("âœ… Google Sheets API ì¸ì¦ ì„±ê³µ.")
        except Exception as e:
            st.error(f"âŒ Google Sheets ì¸ì¦ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            st.error(f"`{SERVICE_ACCOUNT_FILE}` íŒŒì¼ì´ `app.py`ì™€ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return pd.DataFrame()
        # --- [ìˆ˜ì • ì™„ë£Œ] ---
        
        all_data_frames = []
        print(f"Google Sheets íŒŒì¼({SPREADSHEET_ID})ì„ ì—½ë‹ˆë‹¤...")
        
        spreadsheet = gc.open_by_key(SPREADSHEET_ID)
        
        print(f"--- '{spreadsheet.title}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ---")
        worksheets = spreadsheet.worksheets()
        
        for worksheet in worksheets:
            if worksheet.title.lower() not in ["sheet1", "template", "mapping"]:
                try:
                    print(f"  '{worksheet.title}' ì‹œíŠ¸ ì½ëŠ” ì¤‘...")
                    data_records = worksheet.get_all_records()
                    if data_records:
                        df_sheet = pd.DataFrame(data_records)
                        df_sheet["ë‚ ì§œ"] = worksheet.title
                        all_data_frames.append(df_sheet)
                except Exception as e:
                    st.warning(f"'{worksheet.title}' ì‹œíŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if not all_data_frames:
            st.error("ë°ì´í„°ê°€ ìˆëŠ” ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        print("ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©í•©ë‹ˆë‹¤...")
        df = pd.concat(all_data_frames, ignore_index=True)
        
        required_cols = ["ì ‘ìˆ˜ ì¹´í…Œê³ ë¦¬", "ìƒë‹´ì œëª©", "ë¬¸ì˜ë‚´ìš©", "taglist"]
        if not all(col in df.columns for col in required_cols):
            st.error(f"í•„ìˆ˜ ì»¬ëŸ¼({required_cols})ì´ Google Sheetsì— ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        df.rename(columns={'taglist': 'L2 íƒœê·¸'}, inplace=True)
        df["ê²Œì„"] = df["ì ‘ìˆ˜ ì¹´í…Œê³ ë¦¬"].apply(classify_game)
        df["í”Œë«í¼"] = df["ì ‘ìˆ˜ ì¹´í…Œê³ ë¦¬"].apply(classify_platform)
        df["ë‚ ì§œ_dt"] = pd.to_datetime(df["ë‚ ì§œ"], format='%y%m%d', errors='coerce')
        df = df.dropna(subset=["ë‚ ì§œ_dt"])
        df['L1 íƒœê·¸'] = df['L2 íƒœê·¸'].map(L2_TO_L1_MAPPING).fillna('ê¸°íƒ€')
        
        df['GSN(USN)'] = df.apply(extract_gsn_usn, axis=1)
        df['ê¸°ê¸°ì •ë³´'] = df.apply(extract_device_info, axis=1)
        df['ë¬¸ì˜ë‚´ìš©_ìš”ì•½'] = df['ë¬¸ì˜ë‚´ìš©'].apply(truncate_inquiry_content)
        df['ê²€ìƒ‰ìš©_ë¬¸ì˜ë‚´ìš©'] = df['ë¬¸ì˜ë‚´ìš©_ìš”ì•½']
        
        print(f"ì´ {len(df)}ê±´ì˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ.")
        return df

    except Exception as e:
        st.error(f"Google Sheets ì—°ê²° ë˜ëŠ” ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

def clean_text_for_wordcloud(text):
    if not isinstance(text, str): return ""
    text = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£\s]', '', text)
    return text.strip()

def classify_sentiment(text):
    if not isinstance(text, str): return "ì¤‘ë¦½"
    positive_keywords = ["ê°ì‚¬í•©ë‹ˆë‹¤", "ì¢‹ì•„ìš”", "ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤", "í•´ê²°", "ê³ ë§™ìŠµë‹ˆë‹¤"]
    negative_keywords = ["ì§œì¦", "ì˜¤ë¥˜", "í™˜ë¶ˆ", "ì•ˆë¼ìš”", "ì“°ë ˆê¸°", "ì¡°ì‘", "ë¶ˆë§Œ", "ë¬¸ì œ", "íŒ¨ëª°ë¦¼", "ì˜¤ë§"]
    text_lower = text.lower()
    if any(keyword in text_lower for keyword in negative_keywords): return "ë¶€ì •"
    if any(keyword in text_lower for keyword in positive_keywords): return "ê¸ì •"
    return "ì¤‘ë¦½"

def generate_wordcloud(text_data):
    cleaned_texts = [clean_text_for_wordcloud(text) for text in text_data]
    text = ' '.join(cleaned_texts)
    if not text.strip():
        st.info("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    font_path = 'c:/Windows/Fonts/malgun.ttf'
    korean_stopwords = ['ë¬¸ì˜', 'ê²Œì„', 'í”¼ë§', 'ê³ ê°', 'ë‚´ìš©', 'í™•ì¸', 'ë‹µë³€', 'ë¶€íƒ', 'ì²˜ë¦¬', 'ê´€ë ¨', 'ì•ˆë…•í•˜ì„¸ìš”']
    try:
        wordcloud = WordCloud(
            font_path=font_path, width=800, height=400, background_color='white', stopwords=set(korean_stopwords)
        ).generate(text)
        fig, ax = plt.subplots(figsize=(5, 2.5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    except FileNotFoundError:
        st.warning("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ì„ ìœ„í•œ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (malgun.ttf)")
    except Exception as e: st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def mask_phone_number(text):
    if not isinstance(text, str): return text
    masked_text = re.sub(r'(010[-.\s]?)\d{3,4}([-.\s]?)\d{4}', r'\1****\2****', text)
    return masked_text

# --- 3. UI êµ¬ì„± ---
st.title("ğŸ“Š í”¼ë§ ì›¹ë³´ë“œ VOC ëŒ€ì‹œë³´ë“œ")
st.markdown("---")

voc_data = load_data()

if voc_data.empty:
    st.warning("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Google Sheetsë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    with st.sidebar:
        st.title("ğŸ® VOC ëŒ€ì‹œë³´ë“œ í•„í„°")
        
        st.markdown("### ğŸ•¹ï¸ ê²Œì„ ë° í”Œë«í¼ ì„ íƒ")
        game_filters = {
            "ë‰´ë§ê³ ": ["ë‰´ë§ê³  (ì „ì²´)", "ë‰´ë§ê³  MOB", "ë‰´ë§ê³  PC", "ë‰´ë§ê³  for kakao"],
            "ì„¯ë‹¤": ["ì„¯ë‹¤ (ì „ì²´)", "ì„¯ë‹¤ MOB", "ì„¯ë‹¤ PC", "ì„¯ë‹¤ for kakao"],
            "í¬ì»¤": ["í¬ì»¤ (ì „ì²´)", "í¬ì»¤ MOB", "í¬ì»¤ PC", "í¬ì»¤ for kakao"],
            "ì‡¼ë‹¤ìš´í™€ë¤": ["ì‡¼ë‹¤ìš´í™€ë¤ (ì „ì²´)", "ì‡¼ë‹¤ìš´í™€ë¤ MOB", "ì‡¼ë‹¤ìš´í™€ë¤ PC"],
            "ë‰´ë² ê°€ìŠ¤": ["ë‰´ë² ê°€ìŠ¤ (ì „ì²´)", "ë‰´ë² ê°€ìŠ¤ MOB", "ë‰´ë² ê°€ìŠ¤ PC"],
            "ê¸°íƒ€": ["ê¸°íƒ€"]
        }
        
        all_options_with_groups = [opt for sublist in game_filters.values() for opt in sublist]
        all_child_options = [opt for game, opts in game_filters.items() for opt in (opts[1:] if "(ì „ì²´)" in opts[0] else opts)]
        
        def master_checkbox_callback():
            is_all_selected = st.session_state.get('select_all', False)
            for option in all_options_with_groups:
                st.session_state[option] = is_all_selected

        def group_checkbox_callback(game_key):
            is_group_selected = st.session_state.get(f"{game_key} (ì „ì²´)", False)
            for option in game_filters[game_key][1:]:
                st.session_state[option] = is_group_selected
            update_master_checkbox()

        def child_checkbox_callback(game_key):
            if len(game_filters[game_key]) > 1:
                all_children_selected = all(st.session_state.get(opt, False) for opt in game_filters[game_key][1:])
                st.session_state[f"{game_key} (ì „ì²´)"] = all_children_selected
            update_master_checkbox()

        def update_master_checkbox():
            all_groups_selected = all(st.session_state.get(f"{game} (ì „ì²´)", False) for game, opts in game_filters.items() if len(opts) > 1 and "(ì „ì²´)" in opts[0])
            all_single_games_selected = all(st.session_state.get(opts[0], False) for game, opts in game_filters.items() if len(opts) == 1)
            st.session_state.select_all = all_groups_selected and all_single_games_selected
        
        if 'filters_initialized' not in st.session_state:
            st.session_state.filters_initialized = True
            st.session_state.select_all = True
            for option in all_options_with_groups:
                st.session_state[option] = True
        
        st.checkbox("ì „ì²´", key='select_all', on_change=master_checkbox_callback)

        for game, options in game_filters.items():
            with st.expander(game, expanded=True):
                if len(options) > 1 and "(ì „ì²´)" in options[0]:
                    st.checkbox(options[0], key=options[0], on_change=group_checkbox_callback, args=(game,))
                    for option in options[1:]:
                        st.checkbox(option, key=option, on_change=child_checkbox_callback, args=(game,))
                else:
                    st.checkbox(options[0], key=options[0], on_change=update_master_checkbox)

        selected_options = [option for option in all_child_options if st.session_state.get(option, False)]
        
        st.markdown("---")
        st.markdown("### ğŸ“… ê¸°ê°„ ì„ íƒ")
        def set_date_range(days):
            max_date = voc_data['ë‚ ì§œ_dt'].max().date()
            st.session_state.date_range = (max_date - timedelta(days=days-1), max_date)
        col1, col2 = st.columns(2)
        with col1: st.button("ìµœê·¼ 7ì¼", on_click=set_date_range, args=(7,), use_container_width=True)
        with col2: st.button("ìµœê·¼ 30ì¼", on_click=set_date_range, args=(30,), use_container_width=True)
        if 'date_range' not in st.session_state: set_date_range(7)
        date_range = st.date_input("ì¡°íšŒ ê¸°ê°„:", key='date_range', min_value=voc_data['ë‚ ì§œ_dt'].min().date(), max_value=voc_data['ë‚ ì§œ_dt'].max().date())

    if not selected_options:
        filtered_data = pd.DataFrame()
    else:
        conditions = []
        for option in selected_options:
            if " for kakao" in option:
                game_name = option.replace(" for kakao", "")
                platform_name = "for kakao"
                conditions.append((voc_data['ê²Œì„'] == game_name) & (voc_data['í”Œë«í¼'] == platform_name))
            elif len(option.split(" ")) > 1:
                parts = option.split(" ", 1)
                game_name = parts[0]
                platform_name = parts[1]
                conditions.append((voc_data['ê²Œì„'] == game_name) & (voc_data['í”Œë«í¼'] == platform_name))
            else:
                conditions.append(voc_data['ê²Œì„'] == option)
        if conditions:
            final_condition = pd.concat(conditions, axis=1).any(axis=1)
            filtered_data = voc_data[final_condition].copy()
        else:
            filtered_data = pd.DataFrame()
            
    if not filtered_data.empty and len(date_range) == 2:
        start_date = pd.to_datetime(date_range[0])
        end_date = pd.to_datetime(date_range[1])
        filtered_data = filtered_data[(filtered_data["ë‚ ì§œ_dt"] >= start_date) & (filtered_data["ë‚ ì§œ_dt"] <= end_date)]

    if filtered_data.empty:
        st.warning(f"ì„ íƒí•˜ì‹  ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” VOC ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.header("ğŸš€ í•µì‹¬ ì§€í‘œ ìš”ì•½")
        st.markdown(f"**ê¸°ê°„: {date_range[0].strftime('%Y-%m-%d')} ~ {date_range[1].strftime('%Y-%m-%d')}**")
        current_count = len(filtered_data)
        top3_categories = filtered_data['L2 íƒœê·¸'].value_counts().nlargest(3)
        col1, col2, col3 = st.columns(3)
        col1.metric("ì´ VOC ê±´ìˆ˜", f"{current_count} ê±´")
        # (ì „ì£¼ ëŒ€ë¹„ ë¡œì§ì€ ìƒëµ)
        with col3:
            st.markdown("**ì£¼ìš” ì¹´í…Œê³ ë¦¬ TOP 3**")
            for i, (cat, count) in enumerate(top3_categories.items()):
                st.markdown(f"**{i+1}.** {cat} ({count}ê±´)")
        
        st.markdown("---")
        
        tab_main, tab_search = st.tabs(["ğŸ“Š ì¹´í…Œê³ ë¦¬ ë¶„ì„", "ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰"])
        with tab_main:
            st.header("ğŸ“… ì¼ìë³„ VOC ë°œìƒ ì¶”ì´")
            all_days_in_range = pd.date_range(start=date_range[0], end=date_range[1], freq='D')
            range_df = pd.DataFrame(all_days_in_range, columns=['ë‚ ì§œ_dt'])
            daily_counts = filtered_data.groupby(filtered_data['ë‚ ì§œ_dt'].dt.date).size().reset_index(name="ê±´ìˆ˜")
            daily_counts['ë‚ ì§œ_dt'] = pd.to_datetime(daily_counts['ë‚ ì§œ_dt'])
            merged_daily_data = pd.merge(range_df, daily_counts, on='ë‚ ì§œ_dt', how='left').fillna(0)
            fig_daily_trend = px.line(
                merged_daily_data, x='ë‚ ì§œ_dt', y='ê±´ìˆ˜',
                title=f"<b>{date_range[0]} ~ {date_range[1]} ì¼ìë³„ VOC ì¶”ì´</b>",
                labels={'ë‚ ì§œ_dt': 'ë‚ ì§œ', 'ê±´ìˆ˜': 'VOC ê±´ìˆ˜'}, markers=True, text="ê±´ìˆ˜"
            )
            fig_daily_trend.update_traces(textposition="top center")
            fig_daily_trend.update_layout(xaxis_title="", yaxis_title="ê±´ìˆ˜")
            st.plotly_chart(fig_daily_trend, use_container_width=True)

            st.header("ğŸ“Œ VOC ì¹´í…Œê³ ë¦¬ë³„ í˜„í™©")
            l2_counts = filtered_data['L2 íƒœê·¸'].value_counts().nlargest(10).sort_values(ascending=True)
            fig_l2 = px.bar(
                l2_counts, x=l2_counts.values, y=l2_counts.index, orientation='h', 
                title="<b>íƒœê·¸ë³„ í˜„í™© TOP 10</b>", labels={'x': 'ê±´ìˆ˜', 'y': 'íƒœê·¸'}, text_auto=True
            )
            st.plotly_chart(fig_l2, use_container_width=True)
            
            st.subheader("ğŸ“‘ VOC ì›ë³¸ ë°ì´í„°")
            col1, col2 = st.columns([3, 1])
            with col1:
                all_categories = sorted(filtered_data['L2 íƒœê·¸'].unique())
                selected_categories = st.multiselect("í™•ì¸í•˜ê³  ì‹¶ì€ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:", options=all_categories, default=top3_categories.index.tolist())
            
            if selected_categories:
                display_data = filtered_data[filtered_data['L2 íƒœê·¸'].isin(selected_categories)].copy()
                with col2:
                    st.text(" ") # ë²„íŠ¼ ìœ„ì¹˜ ì¡°ì •ì„ ìœ„í•œ ë¹ˆ ê³µê°„
                    st.download_button(
                        label="ğŸ“¥ CSVë¡œ ë‹¤ìš´ë¡œë“œ",
                        data=display_data.to_csv(index=False).encode('utf-8-sig'),
                        file_name=f"voc_category_data_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                display_data['ë¬¸ì˜ë‚´ìš©_ìš”ì•½'] = display_data['ë¬¸ì˜ë‚´ìš©_ìš”ì•½'].apply(mask_phone_number)
                display_df = display_data.rename(columns={'í”Œë«í¼': 'êµ¬ë¶„', 'ë¬¸ì˜ë‚´ìš©_ìš”ì•½': 'ë¬¸ì˜ ë‚´ìš©'})
                st.dataframe(display_df[["êµ¬ë¶„", "ë‚ ì§œ", "ê²Œì„", "L2 íƒœê·¸", "ìƒë‹´ì œëª©", "ë¬¸ì˜ ë‚´ìš©", "GSN(USN)", "ê¸°ê¸°ì •ë³´"]], use_container_width=True, height=500)

        with tab_search:
            st.header("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")
            search_keyword = st.text_input("ë¶„ì„í•˜ê³  ì‹¶ì€ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: í™˜ë¶ˆ, íŠ•ê¹€, ì—…ë°ì´íŠ¸...")
            if search_keyword:
                search_results = filtered_data[filtered_data["ìƒë‹´ì œëª©"].str.contains(search_keyword, na=False, case=False) | filtered_data["ê²€ìƒ‰ìš©_ë¬¸ì˜ë‚´ìš©"].str.contains(search_keyword, na=False, case=False)].copy()
                if not search_results.empty:
                    st.success(f"âœ… \"{search_keyword}\" í‚¤ì›Œë“œê°€ í¬í•¨ëœ VOC: **{len(search_results)}**ê±´")
                    
                    st.subheader(f"'{search_keyword}' í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì´")
                    search_all_days = pd.date_range(start=date_range[0], end=date_range[1], freq='D')
                    search_range_df = pd.DataFrame(search_all_days, columns=['ë‚ ì§œ_dt'])
                    search_daily_counts = search_results.groupby(search_results['ë‚ ì§œ_dt'].dt.date).size().reset_index(name="ê±´ìˆ˜")
                    search_daily_counts['ë‚ ì§œ_dt'] = pd.to_datetime(search_daily_counts['ë‚ ì§œ_dt'])
                    search_merged_data = pd.merge(search_range_df, search_daily_counts, on='ë‚ ì§œ_dt', how='left').fillna(0)
                    fig_search_trend = px.line(
                        search_merged_data, x='ë‚ ì§œ_dt', y='ê±´ìˆ˜',
                        title=f"<b>'{search_keyword}' í‚¤ì›Œë“œ ì¼ìë³„ ë°œìƒ ì¶”ì´</b>",
                        labels={'ë‚ ì§œ_dt': 'ë‚ ì§œ', 'ê±´ìˆ˜': 'VOC ê±´ìˆ˜'}, markers=True, text="ê±´ìˆ˜"
                    )
                    fig_search_trend.update_traces(textposition="top center")
                    fig_search_trend.update_layout(xaxis_title="", yaxis_title="ê±´ìˆ˜")
                    st.plotly_chart(fig_search_trend, use_container_width=True)

                    search_results['ê°ì„±'] = search_results['ë¬¸ì˜ë‚´ìš©'].apply(classify_sentiment)
                    sentiment_counts = search_results['ê°ì„±'].value_counts()
                    st.subheader("ê°ì„± ë¶„ì„ ê²°ê³¼")
                    sentiment_cols = st.columns(3)
                    sentiment_cols[0].metric("ê¸ì • ğŸ˜Š", f"{sentiment_counts.get('ê¸ì •', 0)} ê±´")
                    sentiment_cols[1].metric("ë¶€ì • ğŸ˜ ", f"{sentiment_counts.get('ë¶€ì •', 0)} ê±´")
                    sentiment_cols[2].metric("ì¤‘ë¦½ ğŸ˜", f"{sentiment_counts.get('ì¤‘ë¦½', 0)} ê±´")
                    
                    st.subheader("ê´€ë ¨ VOC ëª©ë¡")
                    st.download_button(
                        label="ğŸ“¥ ê²€ìƒ‰ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                        data=search_results.to_csv(index=False).encode('utf-8-sig'),
                        file_name=f"voc_search_{search_keyword}_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv"
                    )
                    search_results['ë¬¸ì˜ë‚´ìš©_ìš”ì•½'] = search_results['ë¬¸ì˜ë‚´ìš©_ìš”ì•½'].apply(mask_phone_number)
                    display_search_df = search_results.rename(columns={'í”Œë«í¼': 'êµ¬ë¶„', 'ë¬¸ì˜ë‚´ìš©_ìš”ì•½': 'ë¬¸ì˜ ë‚´ìš©'})
                    st.dataframe(display_search_df[["êµ¬ë¶„", "ë‚ ì§œ", "ê²Œì„", "L2 íƒœê·¸", "ìƒë‹´ì œëª©", "ë¬¸ì˜ ë‚´ìš©", "GSN(USN)", "ê¸°ê¸°ì •ë³´", "ê°ì„±"]], use_container_width=True, height=400)
                    
                    st.subheader("ì—°ê´€ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
                    generate_wordcloud(search_results["ë¬¸ì˜ë‚´ìš©"])
                else:
                    st.warning(f"âš ï¸ \"{search_keyword}\" í‚¤ì›Œë“œê°€ í¬í•¨ëœ VOCê°€ ì—†ìŠµë‹ˆë‹¤.")

